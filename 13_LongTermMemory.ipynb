{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short-Term vs. Long-Term Memory\n",
    "\n",
    "Gli esseri umani gestiscono la loro memoria in due modi short-term e long-term.\n",
    "\n",
    "Immaginiamo di avere una conversazione con un amico, la nostra short-term memory richiama dettagli recenti come \"Ho appena passato il sale al mio amico\". Invece, la nostra long-term memory trattiene le informazioni chiave nel tempo, come \"sapere che al nostro amico preferisce il the al posto del caffè o che ama il cibi italiano\". Non ricordedremo ogni conversazione avuta parola per parola.\n",
    "\n",
    "Tratterremo i dettagli importanti che formere interazioni future.\n",
    "\n",
    "Questi concetti vengono applicati anche in LangGraph.\n",
    "\n",
    "![alt text](memory.png)\n",
    "\n",
    "## Short-Term Memory \n",
    "\n",
    "Questa memoria ha come scopo di trattenere in memoria una singola session. In LangGraph la short-term memory è gestita utilizzando i checkpoints i quali memorizzano i dati transitori come l'alternarsi di Human e AI messages all'interno di una conversazione.\n",
    "\n",
    "Tuttavia, ciascuna conversazione è isolata si hanno, conversation 1, conversation 2, conversation 3, ecc. e non vengono memorizzate in maniera persistente oltre alla loro sessione.\n",
    "\n",
    "## Long-Term Memory\n",
    "\n",
    "Questa memoria memorizza in maniera persistente non solo durante la sessione individuale. Questa permette agli agenti di richiamare e riutilizzare informazioni da molteplici conversazioni. \n",
    "\n",
    "Questo consente la cross-thread memory (memoria tra thread), la quale garantisce che gli agenti possano adattare le preferenze degli utenti e mantenere continuità nel tempo.\n",
    "\n",
    "LangGraph introduce lo store object per questo scopo, abilitanto la durabilità della memorizzazione dei dati e il loro recupero ongi volta che è necessario.\n",
    "\n",
    "![alt text](store.png)\n",
    "\n",
    "## Combinazione di entrambe le memorie\n",
    "\n",
    "Combinando shot-term e long-term memory, LangGraph equipaggia gli sviluppatori di creare agenti che sono context-aware durante singole conversazioni e capaci di sfruttare la conoscenza persistente per migliorare le interazioni lungo le sessioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-Term Memory\n",
    "\n",
    "LangGraph fornisce InMemoryStore class.\n",
    "\n",
    "Dopo aver istanziato un oggetto, possiamo usare il metodo put per memorizzare qualcosa in questa long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "user_id = \"my-user\"\n",
    "application_context = \"chitchat\"\n",
    "namespace = (user_id, application_context)\n",
    "# memorizziamo all'interno di un namespace, la coppia \"a-memory\": {\"rules\": dati che vogliamo memorizzare}\n",
    "store.put(namespace, \"a-memory\", {\"rules\": [\"User likes short, direct language\", \"User only speaks English & Python\"], \"my-key\": \"my-value\"})\n",
    "store.put(namespace=namespace, key=\"another-memory\", value={\"rules\": [\"User prefers concise answers\"], \"my-key\": \"my-value\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rules': ['User likes short, direct language',\n",
       "  'User only speaks English & Python'],\n",
       " 'my-key': 'my-value'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ora vogliamo recuperare la informazione\n",
    "store.get(namespace, \"a-memory\").value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<langgraph.store.base.Item at 0x27da5683f60>,\n",
       " <langgraph.store.base.Item at 0x27da5745260>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# possiamo anche cercare l'informazione tra le moteplici chiavi (a-memory, another-memory)\n",
    "# vediamo che ci da i due Items che hanno tali valori (a-memory e another-memory)\n",
    "results = store.search(namespace,  filter={\"my-key\": \"my-value\"})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rules': ['User likes short, direct language', 'User only speaks English & Python'], 'my-key': 'my-value'}\n",
      "{'rules': ['User prefers concise answers'], 'my-key': 'my-value'}\n"
     ]
    }
   ],
   "source": [
    "for item in results:\n",
    "    print(item.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal \n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import uuid\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str):\n",
    "    \"\"\"Get the weather at a specific location\"\"\"\n",
    "    if location.lower() in [\"munich\"]:\n",
    "        return \"It's 15 degrees Celsius and cloudy.\"\n",
    "    else:\n",
    "        return \"It's 32 degrees Celsius and sunny.\"\n",
    "\n",
    "tools = [get_weather]\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState, config: dict, *, store: BaseStore):\n",
    "    user_id = config.get(\"configurable\", {}).get(\"user_id\", \"default_user\")\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace) # tutti gli items (cioè le chiavi delle memorie per questo namespace)\n",
    "    info = \"\\n\".join([d.value[\"data\"] for d in memories])\n",
    "    system_msg = f\"You are a helpful assistant.\"\n",
    "    if info:\n",
    "        system_msg += f\" User info:\\n{info}\"\n",
    "    print(\"System Message:\", system_msg)\n",
    "    messages = state['messages']\n",
    "\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    if \"remember\" in last_message.content.lower():\n",
    "        # togliamo la parola \"remember\" dalla frase e teniemo il resto\n",
    "        # questo sarà il contenuto che vogliamo mettere nello store\n",
    "        memory_content = last_message.content.lower().split(\"remember\", 1)[1].strip()\n",
    "        if memory_content:\n",
    "            memory = memory_content\n",
    "            store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n",
    "\n",
    "\n",
    "    # da qui è uguale come facevamo prima\n",
    "\n",
    "    # alla memoria persistente di un user_id messa nel system_message aggiungo\n",
    "    # i messaggi della chat corrente\n",
    "    model_input_messages = [SystemMessage(content=system_msg)] + messages    \n",
    "    response = model.invoke(model_input_messages)  \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ora abbiamo la long-term memory \n",
    "# facciamo anche la short-term memory con i checkpointer\n",
    "\n",
    "checkpointer = MemorySaver()  # short-term memory\n",
    "store = InMemoryStore()  # long-term memory\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\", \n",
    "    should_continue\n",
    ")\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# nella .compile() function ora abbiamo sue argomenti\n",
    "# il primo checkpointer è per la short-term memory\n",
    "# il secondo store è per la long-term memory \n",
    "# così integriamo le due memorie nel grafo\n",
    "graph = workflow.compile(checkpointer=checkpointer, store=store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get information accross multiple threads\n",
    "\n",
    "Dato che memorizzaimo i contenuti dei messaggi (che contengono la parola \"remenber\") di un medesimo user_id anche di diversi workflows nella long-term memory (InMemoryStore()), possiamo retrievare il contenuto di tutti messaggi di user_id, inoltre con la short-term memory (MemorySaver()) possiamo prendere i messaggi del workflow corrente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Message: You are a helpful assistant.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Remember my name is Alice.', additional_kwargs={}, response_metadata={}, id='f775d7e6-bacd-4662-aa18-4e1f1d15c9ce'),\n",
       "  AIMessage(content='Got it, Alice! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-56862960-cd01-4832-80d8-0ebc483ab617-0', usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Remember my name is Alice.\")]}, # va nella long-term memory\n",
    "    config = {\"configurable\": {\"user_id\": \"user123\", \"thread_id\": 1}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Message: You are a helpful assistant. User info:\n",
      "my name is alice.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='d404266c-f30b-4626-818c-c2efc7fddb42'),\n",
       "  AIMessage(content='Your name is Alice.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 64, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-676d9a19-62d9-4dfb-992b-1ac7bae4948f-0', usage_metadata={'input_tokens': 64, 'output_tokens': 7, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my name?\")]},\n",
    "    config = {\"configurable\": {\"user_id\": \"user123\", \"thread_id\": 2}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Message: You are a helpful assistant.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='d52f9137-adf5-4aff-8e5f-850f4bac6b9c'),\n",
       "  AIMessage(content=\"I'm sorry, but I don't have access to personal information, including your name. If you'd like to share your name, feel free to do so!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 55, 'total_tokens': 87, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-bb11c309-7a1a-47d2-909d-28d3e4f43a7e-0', usage_metadata={'input_tokens': 55, 'output_tokens': 32, 'total_tokens': 87, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's my name?\")]},\n",
    "    config= {\"configurable\": {\"user_id\": \"userxyz\", \"thread_id\": 3}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerazioni,  Postgres Store\n",
    "\n",
    "Con questo approccio, usando InMemoryStore() class, LangGraph ci consente di implementare una long-term memory tuttavia questo è abbastanza nuovo quindi è utile per testare/sperimentare una soluzione leggera per la long-term memory. Tuttavia per una vera persistenza è consigliato usare un adeguato database.\n",
    "\n",
    "Questa limitazione vale anche per i checkpoints. Per questo motivo passiamo da degli **in-memory** checkpoints ad un **Postgres checkpoints** come abbiamo fatto nella fullstack application.\n",
    "\n",
    "Possiamo fare la stessa cosa per lo Store (long-term memory) utilizzando **Postgres Store**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg import Connection\n",
    "from psycopg.rows import dict_row\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "\n",
    "# connection string\n",
    "# qui facciamo uso del nostro Postgres DB che abbiamo usato per la fullstack application\n",
    "# per utilizzarlo dobbiamo avviare la il docker-compose dove c'è la definizione dell'immagine postgres come servizio \n",
    "# docker compose up\n",
    "con_string = \"postgresql://postgres:postgres@localhost:5433/postgres\"\n",
    "\n",
    "# usiamo psycopg per creare una connection object \n",
    "conn = Connection.connect(\n",
    "    con_string, \n",
    "    autocommit=True,\n",
    "    prepare_threshold=0,\n",
    "    row_factory=dict_row\n",
    ")\n",
    "\n",
    "postgres_store = PostgresStore(conn=conn)\n",
    "\n",
    "postgres_store.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile(checkpointer=checkpointer, store=postgres_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Message: You are a helpful assistant.\n",
      "System Message: You are a helpful assistant. User info:\n",
      "my name is alice\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='6788b157-4dbf-494d-80f4-890fb891ba9e'),\n",
       "  AIMessage(content='Your name is Alice.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 63, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-c746b9ff-535f-4439-af30-4ebe35dea853-0', usage_metadata={'input_tokens': 63, 'output_tokens': 7, 'total_tokens': 70, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Remember my name is Alice\")]},\n",
    "    config={\"configurable\": {\"user_id\": \"user12345\", \"thread_id\": \"x\"}}\n",
    ")\n",
    "\n",
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's my name?\")]},\n",
    "    config={\"configurable\": {\"user_id\": \"user12345\", \"thread_id\": \"y\"}}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
