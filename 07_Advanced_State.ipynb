{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and Output State\n",
    "\n",
    "Quando abbiamo creato il nostro advanced RAG Agent abbiamo notato che l'agente restituisce molta informazione per gli step intermedi come \"proceed_to_generate\" o \"rephrase_count\". Questa informazione va bene per il debugging, ma in generale per la maggior parte delle volte siamo solo interessati nella risposta finale dell'LLM.\n",
    "\n",
    "LangGraph ci consente di fare questo (avere solo la risposta finale dell'LLM) definendo input e output state.\n",
    "\n",
    "Vediamo come funziona in pratica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima definiamo un workflow come prima dove otteniamo troppa informazione pi√π di quanta ci √® necessaria. E poi andiamo a ripulire tale info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o\")\n",
    "model = ChatOllama(base_url=\"http://127.0.0.1:11434\", model=\"qwen2.5:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class ChatMessages(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    llm_calls: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: ChatMessages):\n",
    "    question = state['question']\n",
    "    llm_calls = state.get('llm_calls', 0)\n",
    "    state['llm_calls'] = llm_calls + 1\n",
    "    print(\"LLM_CALLS:\", state['llm_calls'])\n",
    "    response = model.invoke(input=question)\n",
    "    state['answer'] = response.content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END  \n",
    "\n",
    "workflow = StateGraph(ChatMessages)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_CALLS: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Whats the highest mountain i the world?',\n",
       " 'answer': 'The highest mountain in the world is Mount Everest, which stands at an elevation of 8,849 meters (29,031.69 feet) above sea level. It is located in the Mahalangur Himal sub-range of the Himalayas, on the border between Nepal and Tibet (China).',\n",
       " 'llm_calls': 1}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(input={\"question\": \"Whats the highest mountain i the world?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noi siamo solo interessati alla risposta del LLM, quindi dobbiamo scartare \"question\" e \"llm_calls\".\n",
    "\n",
    "Per fare questo dobbiamo definire pi√π state objects.\n",
    "\n",
    "Vediamo che definiamo 4 stati. \n",
    "\n",
    "Abbiamo l'input state, private state (o state intermedio) e output state.\n",
    "\n",
    "Creiamo anche un Overall state object che eredita tutti gli stati, ma non aggiunge nessuna chiave.\n",
    "\n",
    "L'input state passa solo la question all'LLM.\n",
    "\n",
    "Lo stato intermedio deve sapere il numero di LLM calls che sono state fatte.\n",
    "\n",
    "Ma per l'output state siamo solo interessati alla risposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    question: str\n",
    "\n",
    "class PrivateState(TypedDict):\n",
    "    llm_calls: int\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    answer: str\n",
    "\n",
    "# full state object con tutti gli stati \n",
    "class OverallState(InputState, PrivateState, OutputState):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passiamo il nostro full state object e \n",
    "# altri due parametri input e output dove passiamo gli stati di input e output\n",
    "# rispettivamente\n",
    "workflow = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_CALLS: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'The highest mountain in the world is Mount Everest, which stands at an elevation of 8,848.86 meters (29,031.7 feet) above sea level. It is located in the Mahalangur Himal sub-range of the Himalayas on the border between Nepal and Tibet (China).'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passiamo solo lo stato di input \n",
    "# e ci restitur√† solo lo stato di output ovvero \"answer\"\n",
    "graph.invoke({\"question\": \"Whats the highest mountain in the world?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dunque definendo gli stati di input e di output possiamo pulire lo stato ed ottenere solo la parte che ci interessa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Runtime configuration\n",
    "\n",
    "Vediamo come aggiungere la runtime configuration.\n",
    "\n",
    "Questa pu√≤ essere necessaria se vogliamo cambiare LLM al volo senza compilazione del grafo, o se abbiamo users da diversi paesi e vogliamo rispondere agli users nella loro lingua, questo dovrebbe essere fatto senza compilare il grafo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ **Cos'√® `RunnableConfig` in LangChain?**\n",
    "`RunnableConfig` √® una **configurazione opzionale** che puoi passare quando esegui un `Runnable` in LangChain. Serve per **personalizzare il comportamento dell'esecuzione**, come:\n",
    "\n",
    "- **Tracing e Logging** (per il debug e il monitoraggio)\n",
    "- **Gestione di callbacks** (per ricevere notifiche sugli eventi)\n",
    "- **Timeouts** (per evitare esecuzioni troppo lunghe)\n",
    "- **Modifica dei parametri di runtime** (batch processing, streaming, ecc.)\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Esempio base di `RunnableConfig`**\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "# Funzione che simula un task\n",
    "def double(x):\n",
    "    return x * 2\n",
    "\n",
    "# Creiamo un RunnableLambda\n",
    "runnable = RunnableLambda(double)\n",
    "\n",
    "# Config personalizzata con un nome di esecuzione\n",
    "config = RunnableConfig(tags=[\"doubling_function\"])\n",
    "\n",
    "# Eseguiamo con la configurazione\n",
    "result = runnable.invoke(10, config=config)\n",
    "\n",
    "print(result)  # Output: 20\n",
    "```\n",
    "‚úÖ **Qui `tags=[\"doubling_function\"]` permette di tracciare l'esecuzione per debugging.**\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Parametri principali di `RunnableConfig`**\n",
    "`RunnableConfig` supporta diversi parametri, tra cui:\n",
    "\n",
    "### **1Ô∏è‚É£ `tags` - Etichette per tracciare l'esecuzione**\n",
    "```python\n",
    "config = RunnableConfig(tags=[\"classification_pipeline\"])\n",
    "```\n",
    "üí° Utile per tracciare le esecuzioni in logging e debugging.\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ `metadata` - Informazioni aggiuntive**\n",
    "```python\n",
    "config = RunnableConfig(metadata={\"user_id\": 123, \"experiment\": \"test_1\"})\n",
    "```\n",
    "üí° Pu√≤ contenere qualsiasi informazione utile per analisi o monitoraggio.\n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ `max_concurrency` - Numero massimo di esecuzioni simultanee**\n",
    "```python\n",
    "config = RunnableConfig(max_concurrency=5)\n",
    "```\n",
    "üí° Utile per eseguire pi√π istanze dello stesso `Runnable` in parallelo.\n",
    "\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ `callbacks` - Aggiunta di callback per il monitoraggio**\n",
    "Puoi usare i **callback** per monitorare quando un `Runnable` inizia o termina l'esecuzione.\n",
    "\n",
    "```python\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "class MyCallbackHandler(BaseCallbackHandler):\n",
    "    def on_start(self, *args, **kwargs):\n",
    "        print(\"Runnable started!\")\n",
    "\n",
    "    def on_end(self, *args, **kwargs):\n",
    "        print(\"Runnable finished!\")\n",
    "\n",
    "config = RunnableConfig(callbacks=[MyCallbackHandler()])\n",
    "```\n",
    "üí° Ora, ogni volta che esegui il `Runnable`, verranno stampati i messaggi `\"Runnable started!\"` e `\"Runnable finished!\"`.\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Applicazione avanzata: `RunnableConfig` con `RunnableSequence`**\n",
    "Se hai una pipeline di pi√π `Runnable`, puoi passare `RunnableConfig` per monitorare l'intera esecuzione.\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "# Definiamo due funzioni\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "\n",
    "def multiply_by_two(x):\n",
    "    return x * 2\n",
    "\n",
    "# Creiamo una sequenza di Runnable\n",
    "pipeline = RunnableSequence(\n",
    "    RunnableLambda(add_one),\n",
    "    RunnableLambda(multiply_by_two)\n",
    ")\n",
    "\n",
    "# Configurazione con logging\n",
    "config = RunnableConfig(tags=[\"math_pipeline\"])\n",
    "\n",
    "# Eseguiamo la sequenza con la config\n",
    "result = pipeline.invoke(10, config=config)\n",
    "\n",
    "print(result)  # Output: (10 + 1) * 2 = 22\n",
    "```\n",
    "‚úÖ **L'intera pipeline √® tracciata sotto il tag `math_pipeline`.**\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Quando usare `RunnableConfig`?**\n",
    "‚úÖ **Debugging & Tracing** ‚Üí Usa `tags` e `metadata` per analizzare l'esecuzione.  \n",
    "‚úÖ **Parallelismo** ‚Üí Controlla con `max_concurrency`.  \n",
    "‚úÖ **Monitoraggio** ‚Üí Usa `callbacks` per ricevere notifiche sugli eventi.  \n",
    "\n",
    "üí° **Conclusione**: `RunnableConfig` √® utile per personalizzare e monitorare il comportamento dei `Runnable` in LangChain, rendendolo pi√π flessibile e scalabile. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# se vogliamo aggiungere una configurazione runtime, dobbiamo passare \n",
    "# il parametro config alla funzione che definisce il nodo\n",
    "# Per rendere il nostro grafo configurabile, ogni informazione di configurazione\n",
    "# deve essere passata dentro la \"configurable\" key\n",
    "# qui possiamo definire dizionari con chiavi e valori\n",
    "def call_model(state: OverallState, config: RunnableConfig):\n",
    "    # vediamo come configurare la lingua\n",
    "    language = config['configurable'].get(\"language\", \"English\")\n",
    "    # utilizziamo il valore della lingua preso dal dizinario config['configurable']\n",
    "    # nel nostro system_prompt per far si che il modello risponda in una lingua dinamica\n",
    "    system_message_content = \"Respond in {language}\".format(language=language)\n",
    "\n",
    "    system_message = SystemMessage(content=system_message_content)\n",
    "\n",
    "    messages = [system_message, HumanMessage(content=state['question'])]\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    return {\"answer\": response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(ChatMessages)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What's the highest mountain in the world?\",\n",
       " 'answer': AIMessage(content='El Monte Everest es el monte m√°s alto del mundo, con una altura de 8.849 metros sobre el nivel del mar.', additional_kwargs={}, response_metadata={'model': 'qwen2.5:7b', 'created_at': '2025-03-12T09:47:20.5051008Z', 'done': True, 'done_reason': 'stop', 'total_duration': 744974200, 'load_duration': 60515300, 'prompt_eval_count': 25, 'prompt_eval_duration': 18000000, 'eval_count': 30, 'eval_duration': 660000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-e0a12e32-8b9d-4e3c-ad29-6264d2dbdc35-0', usage_metadata={'input_tokens': 25, 'output_tokens': 30, 'total_tokens': 55})}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usiamo la configurazione definendo un oggetto config\n",
    "# e passandolo nel metodo invoke()\n",
    "config = {\"configurable\": {\"language\": \"Spanish\"}} \n",
    "\n",
    "graph.invoke(input={\"question\": \"What's the highest mountain in the world?\"}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
