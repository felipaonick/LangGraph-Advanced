{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async & Streaming\n",
    "\n",
    "Vediamo ora un'argomento neccesario se vogliamo creare dei grafi pronti per la produzione.\n",
    "\n",
    "## Async\n",
    "\n",
    "Fin'ora per eseguire un grafo, abbiamo solamente usato .invoke(), che è un metodo sincrono, questo può causare ritardi significativi quando gestiamo molteplici richieste dato che Python esegue in un singolo THREAD.\n",
    "\n",
    "Vediamo meglio cosa vuol dire:\n",
    "\n",
    "Con synchronous code, quando un User A fa una richiesta, il sistema invia una richiesta al modello e attende per la risposta. Durante questo tempo di attesa, il THREAD è bloccato e non può gestire un'altra richiesta.\n",
    "\n",
    "Ad esempio, se avessimo avuto 5 richieste di cui ciascuna prende 2 secondi per processare il sistema le processa una dopo l'altra, risultando in totale di 10 secondi.\n",
    "\n",
    "![alt text](sync.png)\n",
    "\n",
    "Con un codice asincrono, un User A fa una richiesta, ma il THREAD non si blocca nell'attesa che la richiesta dell'user A venga risposta.\n",
    "\n",
    "Ad esempio, quando l'user A invia una richiesta al modello, il sistema può immediatamente gestire altre richieste invece di sedersi in attesa che il modello risponda alla prima richiesta. Invece, il processo cicla lungo le richieste, facendo un avanzamento su tutte esse.\n",
    "\n",
    "In questo modo, tutte le 5 richieste possono essere processate in concorrenza, completandole in circa 2 secondi.\n",
    "\n",
    "![alt text](async.png)\n",
    "\n",
    "\n",
    "Quando rilasciamo in produzione, un agente che gestisce molte richieste utilizzando codice asincrono è assulutamente essenziale. Is MANDATORY for real world applications!\n",
    "\n",
    "Questo assicura che l'applicazione rimanga responsive ed efficiente anche sotto carico.\n",
    "\n",
    "## Streaming\n",
    "\n",
    "Un'altra tecnica spesso usata in applicazioni condivise è lo streaming.\n",
    "\n",
    "Senza lo streaming, l'utente deve attendere che l'agente abbia generato l'intera risposta prima di vedere qualsiasi cosa nell'UI.\n",
    "\n",
    "Questo può portare a ritardi, specialmente con output lunghi, e l'utente potrebbe chiudere l'applicazione.\n",
    "\n",
    "Con lo streaming abilitato, la risposta è inviata all'utente in modo incrementale, token by token mentre viene generata.\n",
    "\n",
    "Questo consente all'utente di vedere l'output venendo creato in real-time, risultando più fluido e veloce.\n",
    "\n",
    "Tuttavia, streaming diventa più complicato in workflows che coinvolgono molteplici agenti.\n",
    "\n",
    "In questi casi, non vogliamo streammare immediatamente i risultati, come dati parzialmente processati o decisioni fatte dall'agente.\n",
    "\n",
    "Questo significa che l'user deve ancora aspettare che l'intero workflow sia completato prima di ricevere la risposta.\n",
    "\n",
    "Quindi, mentre lo streaming è incredibilmente utile per agneti conversazionali semplici, può essere non pratico per workflow avanzati dove il risultato finale è l'unico output rilevante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa creiamo un grafo semplice che non fa uso di streaming nè di operazioni ascinrone.\n",
    "\n",
    "In modo da vedere cosa dobbiamo cambiare per farle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Literal\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather(location: str):\n",
    "    \"\"\"Call to get the current weather.\"\"\"\n",
    "    if location.lower() in [\"munich\"]:\n",
    "        return \"It's 15 degrees Celsius and cloudy.\"\n",
    "    else:\n",
    "        return \"It's 32 degrees Celsius and sunny.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather]\n",
    "model = ChatOpenAI(model=\"gpt-4o\").bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\"agent\", should_continue)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='How is the weather in munich?', additional_kwargs={}, response_metadata={}, id='e78880be-96eb-4719-b5ec-d56b61368ac9'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ldiWQ6RF5Tv7O04bzA9tQcMt', 'function': {'arguments': '{\"location\":\"munich\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 52, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e6a1e731-787e-48fc-b6d0-b4cb83fa174a-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'munich'}, 'id': 'call_ldiWQ6RF5Tv7O04bzA9tQcMt', 'type': 'tool_call'}], usage_metadata={'input_tokens': 52, 'output_tokens': 16, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content=\"It's 15 degrees Celsius and cloudy.\", name='get_weather', id='db60ab29-113e-44ac-92ad-4fb4b8d5f94a', tool_call_id='call_ldiWQ6RF5Tv7O04bzA9tQcMt'),\n",
       "  AIMessage(content='The current weather in Munich is 15 degrees Celsius and cloudy.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 83, 'total_tokens': 98, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-270140bc-b9b4-4efb-b443-7f56d02a61e1-0', usage_metadata={'input_tokens': 83, 'output_tokens': 15, 'total_tokens': 98, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"How is the weather in munich?\")]},\n",
    "    config = {\"configurable\": {\"thread_id\": 1}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='How is the weather in munich?', additional_kwargs={}, response_metadata={}, id='e78880be-96eb-4719-b5ec-d56b61368ac9'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ldiWQ6RF5Tv7O04bzA9tQcMt', 'function': {'arguments': '{\"location\":\"munich\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 52, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e6a1e731-787e-48fc-b6d0-b4cb83fa174a-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'munich'}, 'id': 'call_ldiWQ6RF5Tv7O04bzA9tQcMt', 'type': 'tool_call'}], usage_metadata={'input_tokens': 52, 'output_tokens': 16, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content=\"It's 15 degrees Celsius and cloudy.\", name='get_weather', id='db60ab29-113e-44ac-92ad-4fb4b8d5f94a', tool_call_id='call_ldiWQ6RF5Tv7O04bzA9tQcMt'),\n",
       "  AIMessage(content='The current weather in Munich is 15 degrees Celsius and cloudy.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 83, 'total_tokens': 98, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-270140bc-b9b4-4efb-b443-7f56d02a61e1-0', usage_metadata={'input_tokens': 83, 'output_tokens': 15, 'total_tokens': 98, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='What would you recommend to do in that city then?', additional_kwargs={}, response_metadata={}, id='c4fba5e4-b1e3-40a5-9192-dc0d201d5885'),\n",
       "  AIMessage(content=\"With the current weather in Munich being 15 degrees Celsius and cloudy, here are some activities you might enjoy:\\n\\n1. **Visit Museums**: Munich has a rich cultural scene with numerous museums. You can visit the Deutsches Museum, which is one of the world's largest science and technology museums, or explore the Bavarian National Museum for some history and art.\\n\\n2. **Explore Marienplatz**: This central square is great for sightseeing and people-watching. You can see the famous Glockenspiel and explore nearby shops and cafes.\\n\\n3. **Visit Nymphenburg Palace**: This beautiful Baroque palace is a great place to explore, and its gardens are lovely for a stroll even if it's cloudy.\\n\\n4. **English Garden**: If you enjoy nature, the English Garden is one of the largest urban parks in the world. It's perfect for a walk or a bike ride.\\n\\n5. **Enjoy Bavarian Cuisine**: Visit a traditional beer hall like Hofbräuhaus for some authentic Bavarian food and beer.\\n\\n6. **Shop at Viktualienmarkt**: This daily food market offers a variety of fresh produce, meats, cheeses, and more. It's a great place to grab a bite or shop for local specialties.\\n\\n7. **Visit Churches**: Munich has several beautiful churches, including the Frauenkirche and Asam Church, which are worth a visit.\\n\\nThese activities can be enjoyed regardless of the weather, but make sure to bring a jacket as it might get chilly.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 305, 'prompt_tokens': 115, 'total_tokens': 420, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-862be7f1-8d9c-4761-930f-99fb0e2502b0-0', usage_metadata={'input_tokens': 115, 'output_tokens': 305, 'total_tokens': 420, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# follow-up\n",
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What would you recommend to do in that city then?\")]},\n",
    "    config = {\"configurable\": {\"thread_id\": 1}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting production ready - async and streaming \n",
    "\n",
    "Per prima cosa, vogliamo usare lo streaming, ma non tutti i modelli lo supportano.\n",
    "\n",
    "OpenAI si, basta passare il parametro setraming=True.\n",
    "\n",
    "Per rendere il nmostro agente asincrono, dobbiamo cambiare la funzione di definizione e come lo invochiamo il modello.\n",
    "\n",
    "Dunque se vogliamo usare la invocazione asicrona del modello, allora dobbiamo usare il metodo **ainvoke()**, che permette l'utilizzo del codice async.\n",
    "\n",
    "Inoltre, per rendere il codeice asincrono, dobbiamo definre una funzione async, quindi dobbiamo usare **async def** al posto di solo def per definire la nostra funzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\", streaming=True).bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = await model.ainvoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    path_map={\n",
    "        \"tools\": \"tools\", \n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"How is the weather in Munich?\")]}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": 2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='How is the weather in Munich?', additional_kwargs={}, response_metadata={}, id='683b6d24-f44a-4c87-97a4-9c2be647557a'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_WXelB9dLnaZ5E0AdnU1pSc0Q', 'function': {'arguments': '{\"location\":\"Munich\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8'}, id='run-4e47c833-4d65-430b-b99e-3d9f79598b25-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'Munich'}, 'id': 'call_WXelB9dLnaZ5E0AdnU1pSc0Q', 'type': 'tool_call'}]),\n",
       "  ToolMessage(content=\"It's 15 degrees Celsius and cloudy.\", name='get_weather', id='4df61a80-5195-46b8-b218-7e0205971974', tool_call_id='call_WXelB9dLnaZ5E0AdnU1pSc0Q'),\n",
       "  AIMessage(content='The weather in Munich is currently 15 degrees Celsius and cloudy.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8'}, id='run-eb3ac7c6-b2e1-496d-ab65-36b3fcb89aab-0')]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ora invochiamo il nostro grafo asicronomamente\n",
    "# nel caso fosse pubblico può gestire le invoccazioni del nostro grafo contemporeamente\n",
    "# invece di far attandere una richiesta di invocazione del grafo finche \n",
    "# non risponde a quella precedente\n",
    "await graph.ainvoke(\n",
    "    input=inputs,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora performiamo lo streaming effettivamente.\n",
    "\n",
    "In sostanza abbiamo due modi per streammare gli output dal grafo:\n",
    "1. stream_mode = \"update\" che streamma specifici stream changes fatto da ciascun nodo.\n",
    "\n",
    "2. stream_mode = \"messages\" che streamma token by token outputs dalle invocazioni del chat model.\n",
    "\n",
    "La funzione graph.astream() crea un interable e quindi possiamo ciclare quello che crea.\n",
    "\n",
    "Con una applicazione che fa uso di una UI, normalmente si usa \"messages\" come stream_mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output from node 'agent':\n",
      "---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_cICAgzohnTyeXhVgpY0K6UQ7)\n",
      " Call ID: call_cICAgzohnTyeXhVgpY0K6UQ7\n",
      "  Args:\n",
      "    location: Munich\n",
      "None\n",
      "\n",
      "Output from node 'tools':\n",
      "---\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_weather\n",
      "\n",
      "It's 15 degrees Celsius and cloudy.\n",
      "None\n",
      "\n",
      "Output from node 'agent':\n",
      "---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The weather in Munich is currently 15 degrees Celsius and cloudy.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"How is the weather in Munich?\")]}\n",
    "\n",
    "async for output in graph.astream(inputs, stream_mode=\"updates\", config=config):\n",
    "    # stream_mode=\"updates\" yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"\\nOutput from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value['messages'][-1].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's 15 degrees Celsius and cloudy.|The weather in Munich is currently 15 degrees Celsius and cloudy.|"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessageChunk, HumanMessage\n",
    "\n",
    "\n",
    "inputs = [HumanMessage(content=\"How is the weather in Munich?\")]\n",
    "gathered = None\n",
    "\n",
    "async for msg, metadata in graph.astream({\"messages\": inputs}, stream_mode=\"messages\", config=config):\n",
    "    if msg.content and not isinstance(msg, HumanMessage):\n",
    "        # I messaggi che crea graph.astream(stream_mode=\"messages\")\n",
    "        # sono AIMessageChunk, quindi prendiamo token by token\n",
    "        # Print each token as it streams in \n",
    "        # il parametro flush è True altrimenti non\n",
    "        # potremo vedere lo streaming mechanism live\n",
    "        print(msg.content, end=\"|\", flush=True)\n",
    "\n",
    "    # Memorizziamo anche i final results di tale messaggio prodotto dall'llm\n",
    "    # nella variabile gathered\n",
    "    # Handle the AI message chunks for proper assembly \n",
    "    if isinstance(msg, AIMessageChunk):\n",
    "        if gathered is None:\n",
    "            gathered = msg.content\n",
    "        else:\n",
    "            gathered = gathered + msg.content \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(gathered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
